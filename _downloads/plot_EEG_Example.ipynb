{
  "nbformat_minor": 0,
  "nbformat": 4,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "pygments_lexer": "ipython3",
      "version": "3.5.2",
      "name": "python",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "nbconvert_exporter": "python"
    }
  },
  "cells": [
    {
      "source": [
        "%matplotlib inline"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n# Epilepsy Detection Using EEG Data\n\n\nIn this example we'll use the |cesium|_ library to compare\nvarious techniques for epilepsy detection using a classic EEG time series dataset from\n`Andrzejak et al.  <http://www.meb.uni-bonn.de/epileptologie/science/physik/eegdata.html>`_.\nThe raw data are separated into five classes: Z, O, N, F, and S; we will consider a\nthree-class classification problem of distinguishing normal (Z, O), interictal (N, F), and\nictal (S) signals.\n\nThe overall workflow consists of three steps: first, we \"featurize\" the time series by\nselecting some set of mathematical functions to apply to each; next, we build some\nclassification models which use these features to distinguish between classes;\nfinally, we validate our models by generating predictions for some unseen\nholdout set and comparing them to the true class labels.\n\nFirst, we'll load the data and inspect a representative time series from each class:\n\n.. |cesium| replace:: ``cesium``\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn; seaborn.set()\n\nfrom cesium import datasets\n\neeg = datasets.fetch_andrzejak()\n\n# Group together classes (Z, O), (N, F), (S) as normal, interictal, ictal\neeg[\"classes\"] = eeg[\"classes\"].astype('U16') #  allocate memory for longer class names\neeg[\"classes\"][np.logical_or(eeg[\"classes\"]==\"Z\", eeg[\"classes\"]==\"O\")] = \"Normal\"\neeg[\"classes\"][np.logical_or(eeg[\"classes\"]==\"N\", eeg[\"classes\"]==\"F\")] = \"Interictal\"\neeg[\"classes\"][eeg[\"classes\"]==\"S\"] = \"Ictal\"\n\nfig, ax = plt.subplots(1, len(np.unique(eeg[\"classes\"])), sharey=True)\nfor label, subplot in zip(np.unique(eeg[\"classes\"]), ax):\n    i = np.where(eeg[\"classes\"] == label)[0][0]\n    subplot.plot(eeg[\"times\"][i], eeg[\"measurements\"][i])\n    subplot.set(xlabel=\"time (s)\", ylabel=\"signal\", title=label)"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Featurization\n-------------\nOnce the data is loaded, we can generate features for each time series using the\n|cesium.featurize|_ module. The ``featurize`` module includes many built-in\nchoices of features which can be applied for any type of time series data;\nhere we've chosen a few generic features that do not have any special\nbiological significance.\n\nBy default, the time series will featurized in parallel using the\n``dask.multiprocessing`` scheduler; other approaches, including serial and\ndistributed approaches, can be implemented by passing in other ``dask``\nschedulers as the ``get`` argument to ``featurize_time_series``.\n\n.. |cesium.featurize| replace:: ``cesium.featurize``\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "from cesium import featurize\nfeatures_to_use = ['amplitude',\n                   'percent_beyond_1_std',\n                   'maximum',\n                   'max_slope',\n                   'median',\n                   'median_absolute_deviation',\n                   'percent_close_to_median',\n                   'minimum',\n                   'skew',\n                   'std',\n                   'weighted_average']\nfset_cesium = featurize.featurize_time_series(times=eeg[\"times\"],\n                                              values=eeg[\"measurements\"],\n                                              errors=None,\n                                              features_to_use=features_to_use,\n                                              targets=eeg[\"classes\"])\nprint(fset_cesium)"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "The output of ``featurize_time_series`` is an ``xarray.Dataset`` which contains all\nthe feature information needed to train a machine learning model: feature\nvalues are stored as data variables, and the time series index/class label are\nstored as coordinates (a ``channel`` coordinate will also be used later for\nmulti-channel data).\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Custom feature functions\n~~~~~~~~~~~~~~~~~~~~~~~~\nCustom feature functions not built into ``cesium`` may be passed in using the\n``custom_functions`` keyword, either as a dictionary ``{feature_name: function}``, or as a\n`dask graph <http://dask.pydata.org/en/latest/custom-graphs.html>`_. Functions should take\nthree arrays ``times, measurements, errors`` as inputs; details can be found in the\n``cesium.featurize``\n`documentation <http://cesium.ml/docs/api/cesium.featurize.html>`_.\nHere we'll compute five standard features for EEG analysis provided by\n`Guo et al. (2012) <http://linkinghub.elsevier.com/retrieve/pii/S0957417411003253)>`_:\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "import numpy as np\nimport scipy.stats\n\ndef mean_signal(t, m, e):\n    return np.mean(m)\n\ndef std_signal(t, m, e):\n    return np.std(m)\n\ndef mean_square_signal(t, m, e):\n    return np.mean(m ** 2)\n\ndef abs_diffs_signal(t, m, e):\n    return np.sum(np.abs(np.diff(m)))\n\ndef skew_signal(t, m, e):\n    return scipy.stats.skew(m)"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Now we'll pass the desired feature functions as a dictionary via the\n``custom_functions`` keyword argument.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "guo_features = {\n    'mean': mean_signal,\n    'std': std_signal,\n    'mean2': mean_square_signal,\n    'abs_diffs': abs_diffs_signal,\n    'skew': skew_signal\n}\n\nfset_guo = featurize.featurize_time_series(times=eeg[\"times\"], values=eeg[\"measurements\"],\n                                           errors=None, targets=eeg[\"classes\"],\n                                           features_to_use=list(guo_features.keys()),\n                                           custom_functions=guo_features)\nprint(fset_guo)"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Multi-channel time series\n~~~~~~~~~~~~~~~~~~~~~~~~~\nThe EEG time series considered here consist of univariate signal measurements along a\nuniform time grid. But ``featurize_time_series`` also accepts multi-channel\ndata; to demonstrate this, we will decompose each signal into five frequency\nbands using a discrete wavelet transform as suggested by\n`Subasi (2005) <http://www.sciencedirect.com/science/article/pii/S0957417404001745>`_,\nand then featurize each band separately using the five functions from above.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "import pywt\n\nn_channels = 5\neeg[\"dwts\"] = [pywt.wavedec(m, pywt.Wavelet('db1'), level=n_channels-1)\n               for m in eeg[\"measurements\"]]\nfset_dwt = featurize.featurize_time_series(times=None, values=eeg[\"dwts\"], errors=None,\n                                           features_to_use=list(guo_features.keys()),\n                                           targets=eeg[\"classes\"],\n                                           custom_functions=guo_features)\nprint(fset_dwt)"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "The output featureset has the same form as before, except now the ``channel``\ncoordinate is used to index the features by the corresponding frequency band.\nThe functions in ``cesium.build_model`` and ``cesium.predict`` all accept\nfeaturesets from single- or multi-channel data, so no additional steps are\nrequired to train models or make predictions for multichannel featuresets\nusing the ``cesium`` library.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Model Building\n--------------\nModel building in ``cesium`` is handled by the ``build_model_from_featureset``\nfunction in the ``cesium.build_model`` submodule. The featureset output by\n``featurize_time_series`` contains both the feature and target information\nneeded to train a model; ``build_model_from_featureset`` is simply a wrapper\nthat calls the ``fit`` method of a given ``scikit-learn`` model with the\nappropriate inputs. In the case of multichannel features, it also handles\nreshaping the featureset into a (rectangular) form that is compatible with\n``scikit-learn``.\n\nFor this example, we'll test a random forest classifier for the built-in\n``cesium`` features, and a 3-nearest neighbors classifier for the others, as\nsuggested by\n`Guo et al. (2012) <http://linkinghub.elsevier.com/retrieve/pii/S0957417411003253>`_.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "from cesium.build_model import build_model_from_featureset\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(np.arange(len(eeg[\"classes\"])), random_state=0)\n\nrfc_param_grid = {'n_estimators': [8, 16, 32, 64, 128, 256, 512, 1024]}\nmodel_cesium = build_model_from_featureset(fset_cesium.isel(name=train),\n                                          RandomForestClassifier(max_features='auto',\n                                                                 random_state=0),\n                                          params_to_optimize=rfc_param_grid)\nknn_param_grid = {'n_neighbors': [1, 2, 3, 4]}\nmodel_guo = build_model_from_featureset(fset_guo.isel(name=train),\n                                        KNeighborsClassifier(),\n                                        params_to_optimize=knn_param_grid)\nmodel_dwt = build_model_from_featureset(fset_dwt.isel(name=train),\n                                        KNeighborsClassifier(),\n                                        params_to_optimize=knn_param_grid)"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Prediction\n----------\nMaking predictions for new time series based on these models follows the same\npattern: first the time series are featurized using ``featurize_time_series``,\nand then predictions are made based on these features using\n``predict.model_predictions``.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "from sklearn.metrics import accuracy_score\nfrom cesium.predict import model_predictions\n\npreds_cesium = model_predictions(fset_cesium, model_cesium, return_probs=False)\npreds_guo = model_predictions(fset_guo, model_guo, return_probs=False)\npreds_dwt = model_predictions(fset_dwt, model_dwt, return_probs=False)\n\nprint(\"Built-in cesium features: training accuracy={:.2%}, test accuracy={:.2%}\".format(\n          accuracy_score(preds_cesium.prediction.values[train], eeg[\"classes\"][train]),\n          accuracy_score(preds_cesium.prediction.values[test], eeg[\"classes\"][test])))\nprint(\"Guo et al. features: training accuracy={:.2%}, test accuracy={:.2%}\".format(\n          accuracy_score(preds_guo.prediction.values[train], eeg[\"classes\"][train]),\n          accuracy_score(preds_guo.prediction.values[test], eeg[\"classes\"][test])))\nprint(\"Wavelet transform features: training accuracy={:.2%}, test accuracy={:.2%}\".format(\n          accuracy_score(preds_dwt.prediction.values[train], eeg[\"classes\"][train]),\n          accuracy_score(preds_dwt.prediction.values[test], eeg[\"classes\"][test])))"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "The workflow presented here is intentionally simplistic and omits many important steps\nsuch as feature selection, model parameter selection, etc., which may all be\nincorporated just as they would for any other ``scikit-learn`` analysis.\nBut with essentially three function calls (``featurize_time_series``,\n``build_model_from_featureset``, and ``model_predictions``), we are able to build a\nmodel from a set of time series and make predictions on new, unlabeled data. In\nupcoming posts we'll introduce the web frontend for ``cesium`` and describe how\nthe same analysis can be performed in a browser with no setup or coding required.\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}